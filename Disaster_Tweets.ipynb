{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOEnVHB2dh+BWCCntDJl8+v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dyfzl/Disaster-Tweets/blob/main/Disaster_Tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/nlp-getting-started\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.optim import Adam\n"
      ],
      "metadata": {
        "id": "Mn3NX-hPtbzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jThRsojU2YUh",
        "outputId": "5fa063b0-bbf2-4f8a-c523-d09f4521e922"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7Zh0kt42-ti",
        "outputId": "f80e68f5-6103-404f-8f39-024ddd987f31"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "z3NP9p0M3FEU",
        "outputId": "e6e15c42-ee21-40b3-d0e2-3d3f1be5774e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.67.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "iWMcPwbePVRv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13f4f6ec-e7a1-4ff2-ceb3-75c4a464d93c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lower:  True\n",
            "Split:   \n",
            "Filters:  !\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-42-5b8cf60151bd>:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_masks = torch.tensor(train_masks)\n",
            "<ipython-input-42-5b8cf60151bd>:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_masks = torch.tensor(val_masks)\n"
          ]
        }
      ],
      "source": [
        "# 데이터 불러오기 및 전처리\n",
        "pre_data_tr = pd.read_csv('/content/drive/MyDrive/nlp-getting-started/train.csv')\n",
        "train_data = pre_data_tr.sample(frac=1)  # 데이터 셔플\n",
        "pre_data_te = pd.read_csv('/content/drive/MyDrive/nlp-getting-started/test.csv')\n",
        "test_data = pre_data_te.sample(frac=1)  # 데이터 셔플\n",
        "sub_data = pd.read_csv('/content/drive/MyDrive/nlp-getting-started/sample_submission.csv')\n",
        "\n",
        "# BERT 토크나이저 초기화\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import tensorflow as tf\n",
        "\n",
        "# NLTK 불용어 및 Lemmatizer 초기화\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# 데이터 정제 함수\n",
        "def cleaned(text):\n",
        "    # 개행 문자 제거\n",
        "    text = re.sub(r\"\\n\", \"\", text)\n",
        "    # 소문자 변환\n",
        "    text = text.lower()\n",
        "    # 숫자 제거\n",
        "    text = re.sub(r\"\\d\", \"\", text)\n",
        "    # 비 ASCII 문자 제거\n",
        "    text = re.sub(r'[^\\x00-\\x7f]', r' ', text)\n",
        "    # 구두점 제거\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # URL 제거\n",
        "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
        "    # 불용어 제거\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    # Lemmatizer 적용\n",
        "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "    return text\n",
        "\n",
        "# 데이터프레임의 텍스트 칼럼 정제\n",
        "train_data['text'] = train_data['text'].apply(lambda x: cleaned(x))\n",
        "train_data['keyword'] = train_data['keyword'].fillna(\"\")\n",
        "\n",
        "# 검증 데이터 분리\n",
        "val_data = train_data.tail(1500)\n",
        "train_data = train_data.head(len(train_data) - 1500)\n",
        "\n",
        "# 토큰화 함수 정의\n",
        "def define_tokenizer(train_sentences, val_sentences, test_sentences):\n",
        "    sentences = pd.concat([train_sentences, val_sentences, test_sentences])\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "    tokenizer.fit_on_texts(sentences)\n",
        "    return tokenizer\n",
        "\n",
        "def encode(sentences, tokenizer):\n",
        "    encoded_sentences = tokenizer.texts_to_sequences(sentences)\n",
        "    encoded_sentences = tf.keras.preprocessing.sequence.pad_sequences(encoded_sentences, padding='post')\n",
        "    return encoded_sentences\n",
        "\n",
        "# 토크나이저 정의 및 인코딩\n",
        "tokenizer = define_tokenizer(train_data['text'], val_data['text'], test_data['text'])\n",
        "encoded_train_sentences = encode(train_data['text'], tokenizer)\n",
        "encoded_val_sentences = encode(val_data['text'], tokenizer)\n",
        "encoded_test_sentences = encode(test_data['text'], tokenizer)\n",
        "\n",
        "# 토크나이저 설정 확인\n",
        "print('Lower: ', tokenizer.get_config()['lower'])\n",
        "print('Split: ', tokenizer.get_config()['split'])\n",
        "print('Filters: ', tokenizer.get_config()['filters'])\n",
        "\n",
        "\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# PyTorch 텐서로 변환\n",
        "train_labels = torch.tensor(train_data['target'].values)\n",
        "val_labels = torch.tensor(val_data['target'].values)\n",
        "test_labels = torch.tensor([0] * len(test_data))  # 테스트 데이터 레이블은 dummy로 사용\n",
        "\n",
        "# 학습 및 검증 데이터 분리\n",
        "train_inputs, val_inputs, train_masks, val_masks = train_test_split(\n",
        "    encoded_train_sentences, train_labels, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "val_inputs = torch.tensor(val_inputs)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "val_masks = torch.tensor(val_masks)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "train_encodings = tokenizer(list(train_data['keyword'] + ' ' + train_data['text']), truncation=True, padding=True, max_length=128)\n",
        "val_encodings = tokenizer(list(val_data['keyword'] + ' ' + val_data['text']), truncation=True, padding=True, max_length=128)\n",
        "\n",
        "# 1. 결측값 처리\n",
        "test_data['keyword'] = test_data['keyword'].fillna(\"\")\n",
        "test_data['text'] = test_data['text'].fillna(\"\")\n",
        "\n",
        "# 2. 문자열 형식 확인 및 변환\n",
        "test_data['keyword'] = test_data['keyword'].astype(str)\n",
        "test_data['text'] = test_data['text'].astype(str)\n",
        "\n",
        "# 3. 텍스트 결합 및 리스트 변환\n",
        "test_texts = (test_data['keyword'] + ' ' + test_data['text']).tolist()\n",
        "\n",
        "# 4. BERT 토크나이저를 통한 토큰화\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 데이터 재분리\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    (train_data['keyword'] + ' ' + train_data['text']).tolist(),  # 텍스트 데이터\n",
        "    train_data['target'].values,                                 # 레이블\n",
        "    test_size=0.3,                                               # 검증 데이터 비율\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 토큰화\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n",
        "\n",
        "# Tensor로 변환\n",
        "train_inputs = torch.tensor(train_encodings['input_ids'])\n",
        "train_masks = torch.tensor(train_encodings['attention_mask'])\n",
        "train_labels = torch.tensor(train_labels)\n",
        "\n",
        "val_inputs = torch.tensor(val_encodings['input_ids'])\n",
        "val_masks = torch.tensor(val_encodings['attention_mask'])\n",
        "val_labels = torch.tensor(val_labels)\n"
      ],
      "metadata": {
        "id": "TNogFce13y3J"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"train_inputs size: \", train_inputs.size())\n",
        "print(\"train_masks size: \", train_masks.size())\n",
        "print(\"train_labels size: \", train_labels.size())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIEtgOcw3oXh",
        "outputId": "4bee19f7-41da-4593-9f3c-79c20d992bd5"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_inputs size:  torch.Size([4279, 48])\n",
            "train_masks size:  torch.Size([4279, 48])\n",
            "train_labels size:  torch.Size([4279])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TensorDataset 및 DataLoader로 변환\n",
        "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "val_dataset = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "# BERT 분류 모델 초기화\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# 손실 함수와 옵티마이저 설정\n",
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = Adam(model.parameters(), lr=2e-5)\n",
        "\n",
        "# 학습 루프\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # 검증 단계\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            predictions = torch.argmax(logits, dim=1)  # 예측 클래스 선택\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    val_accuracy = correct / total\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_train_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "# 테스트 단계\n",
        "test_inputs = torch.tensor(test_encodings['input_ids'])\n",
        "test_masks = torch.tensor(test_encodings['attention_mask'])\n",
        "\n",
        "test_dataset = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "model.eval()\n",
        "test_pred = []\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=1).cpu()  # 예측 클래스 선택 및 CPU로 이동\n",
        "        test_pred.extend(predictions.numpy())\n",
        "\n",
        "        # 테스트 정확도 계산\n",
        "        correct += (predictions == labels.cpu()).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "# 최종 정확도 계산 및 출력\n",
        "test_accuracy = correct / total\n",
        "print(f\"Final Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "# 최종 제출 파일 생성\n",
        "sub_data = {\n",
        "    'id': test_data['id'].values.astype('int64'),\n",
        "    'target': [int(pred) for pred in test_pred]  # 정수로 변환\n",
        "}\n",
        "submission = pd.DataFrame(sub_data)\n",
        "submission.to_csv(\"/content/drive/MyDrive/nlp-getting-started/submission.csv\", sep=',', float_format='%.0f', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qsp2kcNL3iTu",
        "outputId": "42b1d848-147f-4d40-95ad-b6a2db1452eb"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.4933, Validation Accuracy: 0.8168\n",
            "Epoch 2/10, Loss: 0.3664, Validation Accuracy: 0.8179\n",
            "Epoch 3/10, Loss: 0.2715, Validation Accuracy: 0.8086\n",
            "Epoch 4/10, Loss: 0.1859, Validation Accuracy: 0.8059\n",
            "Epoch 5/10, Loss: 0.1294, Validation Accuracy: 0.7546\n",
            "Epoch 6/10, Loss: 0.0972, Validation Accuracy: 0.8010\n",
            "Epoch 7/10, Loss: 0.0766, Validation Accuracy: 0.8124\n",
            "Epoch 8/10, Loss: 0.0555, Validation Accuracy: 0.7955\n",
            "Epoch 9/10, Loss: 0.0536, Validation Accuracy: 0.7863\n",
            "Epoch 10/10, Loss: 0.0486, Validation Accuracy: 0.8086\n",
            "Final Test Accuracy: 0.7131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 가중치 저장\n",
        "torch.save(model.state_dict(), \"model_checkpoint.pth\")\n",
        "print(\"Model checkpoint saved as 'model_checkpoint.pth'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmZSZJyV-zlB",
        "outputId": "f8adaa9f-fc09-4dfa-c5b0-2fb52a70cfae"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model checkpoint saved as 'model_checkpoint.pth'.\n"
          ]
        }
      ]
    }
  ]
}