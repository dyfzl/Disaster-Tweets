{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgjTCy_j4kns"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.optim import Adam\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/nlp-getting-started\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "Y9Q4iFxW4oIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "w7GIJhXP4omY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install tensorflow"
      ],
      "metadata": {
        "id": "rwsnw3Ua4vef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 불러오기 및 전처리\n",
        "pre_data_tr = pd.read_csv('train.csv')\n",
        "train_data = pre_data_tr.sample(frac=1)  # 데이터 셔플\n",
        "pre_data_te = pd.read_csv('test.csv')\n",
        "test_data = pre_data_te.sample(frac=1)  # 데이터 셔플\n",
        "sub_data = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "# BERT 토크나이저 초기화\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import tensorflow as tf\n",
        "\n",
        "# NLTK 불용어 및 Lemmatizer 초기화\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# 데이터 정제 함수\n",
        "def cleaned(text):\n",
        "    # 개행 문자 제거\n",
        "    text = re.sub(r\"\\n\", \"\", text)\n",
        "    # 소문자 변환\n",
        "    text = text.lower()\n",
        "    # 숫자 제거\n",
        "    text = re.sub(r\"\\d\", \"\", text)\n",
        "    # 비 ASCII 문자 제거\n",
        "    text = re.sub(r'[^\\x00-\\x7f]', r' ', text)\n",
        "    # 구두점 제거\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # URL 제거\n",
        "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
        "    # 불용어 제거\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    # Lemmatizer 적용\n",
        "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "    return text\n",
        "\n",
        "# 데이터프레임의 텍스트 칼럼 정제\n",
        "train_data['text'] = train_data['text'].apply(lambda x: cleaned(x))\n",
        "train_data['keyword'] = train_data['keyword'].fillna(\"\")\n",
        "\n",
        "# 검증 데이터 분리\n",
        "val_data = train_data.tail(1500)\n",
        "train_data = train_data.head(len(train_data) - 1500)\n",
        "\n",
        "# 토큰화 함수 정의\n",
        "def define_tokenizer(train_sentences, val_sentences, test_sentences):\n",
        "    sentences = pd.concat([train_sentences, val_sentences, test_sentences])\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "    tokenizer.fit_on_texts(sentences)\n",
        "    return tokenizer\n",
        "\n",
        "def encode(sentences, tokenizer):\n",
        "    encoded_sentences = tokenizer.texts_to_sequences(sentences)\n",
        "    encoded_sentences = tf.keras.preprocessing.sequence.pad_sequences(encoded_sentences, padding='post')\n",
        "    return encoded_sentences\n",
        "\n",
        "# 토크나이저 정의 및 인코딩\n",
        "tokenizer = define_tokenizer(train_data['text'], val_data['text'], test_data['text'])\n",
        "encoded_train_sentences = encode(train_data['text'], tokenizer)\n",
        "encoded_val_sentences = encode(val_data['text'], tokenizer)\n",
        "encoded_test_sentences = encode(test_data['text'], tokenizer)\n",
        "\n",
        "# 토크나이저 설정 확인\n",
        "print('Lower: ', tokenizer.get_config()['lower'])\n",
        "print('Split: ', tokenizer.get_config()['split'])\n",
        "print('Filters: ', tokenizer.get_config()['filters'])\n",
        "\n",
        "\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# PyTorch 텐서로 변환\n",
        "train_labels = torch.tensor(train_data['target'].values)\n",
        "val_labels = torch.tensor(val_data['target'].values)\n",
        "test_labels = torch.tensor([0] * len(test_data))  # 테스트 데이터 레이블은 dummy로 사용\n",
        "\n",
        "# 학습 및 검증 데이터 분리\n",
        "train_inputs, val_inputs, train_masks, val_masks = train_test_split(\n",
        "    encoded_train_sentences, train_labels, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "val_inputs = torch.tensor(val_inputs)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "val_masks = torch.tensor(val_masks)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "train_encodings = tokenizer(list(train_data['keyword'] + ' ' + train_data['text']), truncation=True, padding=True, max_length=128)\n",
        "val_encodings = tokenizer(list(val_data['keyword'] + ' ' + val_data['text']), truncation=True, padding=True, max_length=128)\n",
        "\n",
        "# 1. 결측값 처리\n",
        "test_data['keyword'] = test_data['keyword'].fillna(\"\")\n",
        "test_data['text'] = test_data['text'].fillna(\"\")\n",
        "\n",
        "# 2. 문자열 형식 확인 및 변환\n",
        "test_data['keyword'] = test_data['keyword'].astype(str)\n",
        "test_data['text'] = test_data['text'].astype(str)\n",
        "\n",
        "# 3. 텍스트 결합 및 리스트 변환\n",
        "test_texts = (test_data['keyword'] + ' ' + test_data['text']).tolist()\n",
        "\n",
        "# 4. BERT 토크나이저를 통한 토큰화\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)\n",
        "\n"
      ],
      "metadata": {
        "id": "Bd45vbbZ4wZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 데이터 재분리\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    (train_data['keyword'] + ' ' + train_data['text']).tolist(),  # 텍스트 데이터\n",
        "    train_data['target'].values,                                 # 레이블\n",
        "    test_size=0.3,                                               # 검증 데이터 비율\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 토큰화\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n",
        "\n",
        "# Tensor로 변환\n",
        "train_inputs = torch.tensor(train_encodings['input_ids'])\n",
        "train_masks = torch.tensor(train_encodings['attention_mask'])\n",
        "train_labels = torch.tensor(train_labels)\n",
        "\n",
        "val_inputs = torch.tensor(val_encodings['input_ids'])\n",
        "val_masks = torch.tensor(val_encodings['attention_mask'])\n",
        "val_labels = torch.tensor(val_labels)\n"
      ],
      "metadata": {
        "id": "4U78eA8b4z_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"train_inputs size: \", train_inputs.size())\n",
        "print(\"train_masks size: \", train_masks.size())\n",
        "print(\"train_labels size: \", train_labels.size())"
      ],
      "metadata": {
        "id": "gPF1r5xU46G2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TensorDataset 및 DataLoader로 변환\n",
        "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "val_dataset = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "# BERT 분류 모델 초기화\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# 손실 함수와 옵티마이저 설정\n",
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = Adam(model.parameters(), lr=2e-5)\n",
        "\n",
        "# 학습 루프\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # 검증 단계\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            predictions = torch.argmax(logits, dim=1)  # 예측 클래스 선택\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    val_accuracy = correct / total\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_train_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "# 테스트 단계\n",
        "test_inputs = torch.tensor(test_encodings['input_ids'])\n",
        "test_masks = torch.tensor(test_encodings['attention_mask'])\n",
        "\n",
        "test_dataset = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "model.eval()\n",
        "test_pred = []\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=1).cpu()  # 예측 클래스 선택 및 CPU로 이동\n",
        "        test_pred.extend(predictions.numpy())\n",
        "\n",
        "        # 테스트 정확도 계산\n",
        "        correct += (predictions == labels.cpu()).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "# 최종 정확도 계산 및 출력\n",
        "test_accuracy = correct / total\n",
        "print(f\"Final Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "# 최종 제출 파일 생성\n",
        "sub_data = {\n",
        "    'id': test_data['id'].values.astype('int64'),\n",
        "    'target': [int(pred) for pred in test_pred]  # 정수로 변환\n",
        "}\n",
        "submission = pd.DataFrame(sub_data)\n",
        "submission.to_csv(\"submission.csv\", sep=',', float_format='%.0f', index=False)\n",
        "print(\"Submission file created successfully.\")"
      ],
      "metadata": {
        "id": "f5wZFAdD46yv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}